# Chapel Hill expert surveys 2019 Data Analysis

## **Project Description**

The Chapel Hill expert surveys estimate party positioning on European integration, ideology and policy issues for national parties in various European countries. 

32 EU countries and 277 political parties' data were collected to show a trend from 1999 to 2019. Chapel hill expert survey(CHES) is the longest-running and most extensive expert survey on political parties in Europe.

Below are the data sets collected and generated by the CHES team.

- [Chapel Hill Expert Survey dataset](https://www.chesdata.eu/s/CHES2019V3.csv) - This is survey datasets which countries, parties and thier orientation on different topics.

- [2019_CHES_codebook](https://static1.squarespace.com/static/5975c9bfdb29d6a05c65209b/t/5fa04ec05d3c8218b7c91450/1604341440585/2019_CHES_codebook.pdf) - This dataset provides the data for the 2019 Chapel Hill Expert Survey on the positioning of 277
political parties on political ideology, European integration, and policy positions in 32 countries,
including all EU member states.
    
    -  This PDF data contains country, country abbreviation, party and party's details such as name, abbreviation, id etc.
     
     
        `party.csv` and `country_abbr.csv` datasets are parsed from the pdf and kept in data folder. Ideal way would be if it is parsed from pdf itself but that is somthing #todo work at later stage.
    -   The pdf also contains orientation details on different features. 
    
        For instance, the column `EU_POSITION` represents the overall orientation of the party leadership towards European integration in 2019.
    
                1 = Strongly opposed
                2 = Opposed
                3 = Somewhat opposed
                4 = Neutral
                5 = Somewhat in favor
                6 = In favor
                7 = Strongly in favor


- [Survey Questionnaire](https://static1.squarespace.com/static/5975c9bfdb29d6a05c65209b/t/5ed3029fe080e33f639e6e9a/1590887075513/CHES_UK_Qualtrics.pdf) - This dataset has general questions on various identified features, and this is the end goal for the given dataset. For example if we see one of the questions which asks- 

    **`How would you describe the general position on European integration that the party leadeship took during 2019?`**

    And then, for each country and their respective political parties, we need to find out the orientation on the scale of 1 to 7 to show how they think on this matter?

    42 such questions exist in this questionnaire which we need to answer.

## **Solution Description**

We now understand the project at a high level that we need to build a report on the questionnaire, and we have few datasets. There are many ways to solve this problem, but here we are more interested in solving it in an Airflow way. 
If you want to check that how can we solve this problem without airflow, then refer to this [repo](python-repo), which solves the problem in the same way in plain python (no airflow)

So, to start with the solution, we have to follow a typical ETL workflow (extract-transform-load). Once all required datasets are loaded and merged correctly, we can build our report on top of that. Solving it in airflow helps us
to leverage several inbuilt features such as logging, tracking, workflow management etc. 

The created dataflow will look like this 

![chapel-hill-survey-dag-graph](/imgs/project_dag_graph.png)

and our visualization will look like this.

![visualization](/imgs/visualization.png)

# Prerequisites
```diff
- Airflow must be installed and working in your machine.
```
Check it out here for [installation guide](https://github.com/saurabh2mishra/airflow-notes#installing-airflow)

# Airflow project strcuture.

Before we start writing our DAG and scripts we must follow the standard folder strcuture to organize
our code base. This is the stanard way to organize our code base.

```tree
───dags
│   ├───common
│   │   ├───hooks
│   │   │       common_hook.py
│   │   │
│   │   ├───operators
│   │   │       common_operator.py
│   │   │       postgres_templated_operator.py
│   │   │
│   │   └───scripts
│   │           scripts.py
│   │
│   ├───project_1
│   │   │   dag_1.py
│   │   │   dag_2.py
│   │   │
│   │   └───sql
│   │        ddls.sql
│   │
│   └───project_2
│       │   dag_1.py
│       │   dag_2.py
│       │
│       └───sql
│             ddls.sql
│
└───data
    ├───project_1
    │   files.csv
    └───project_2
        files.csv
```
Folder structures are followed from this [stackoverflow link](https://stackoverflow.com/questions/44424473/airflow-structure-organization-of-dags-and-tasks).
This is what developers suggest and follow, but it's not mandatory to stick with the same. 
But the idea is to have an organised way to keep the dag and scripts followed and understandable by others.


## DAG Design concerns

Looking at the above DAG design for this project, One might disagree with the workflow and ask

*Why have we organised our tasks in this way? 
Why not use BashOperator to call our python code directly, 
and why not just call the main driver program, which executes 
all these tasks underhood, and we patch them with a single task?
Or why not split them into other tasks?*

These are all valid questions to ask while developing a workflow,
but the truth is, there’s no right or wrong answer. 
There are several points to consider, e.g. idempotency, atomicity, a logical flow, and then it's our choice to design it as per our need.


## Let's understand the project

Code snippets are below

```python
files_uris = {"2019_CHES_codebook.pdf" : config.codebook_uri,
        "CHES2019V3.csv" : config.chesdata_uri, 
        "questionnaire.pdf" :config.questionnaire_uri
        }

for file, uri in enumerate(files_uris):
    file_name = file.split(":")[0
    task_download_data = PythonOperator(
        task_id=f"download_{file_name}",
        python_callable=download.download_files,
        op_kwargs={"uri": uri, "file_name": file_name},
        dag = dag
    )

    # parallelization of all download tasks.
    # This also can be written as start_operator >> task_download_data >> end_download_operators
    
    chain(start_operator, task_download_data, end_download_operators)
```
